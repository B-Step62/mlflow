---
description: MLflow Tracing is a feature that enables LLM observability in your apps. MLflow automatically logs traces for LangChain, LlamaIndex, and more.
sidebar_position: 1
---

import { APILink } from "@site/src/components/APILink";
import { Card, CardGroup, SmallLogoCard } from "@site/src/components/Card";
import TOCInline from "@theme/TOCInline";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# MLflow Tracing for LLM Observability



**MLflow Tracing** is a feature that enhances LLM observability in your Generative AI (GenAI) applications by capturing detailed information about the execution of your application's services.
Tracing provides a way to record the inputs, outputs, and metadata associated with each intermediate step of a request, enabling you to easily pinpoint the source of bugs and unexpected behaviors.

![Tracing Gateway Video](/images/llms/tracing/tracing-top.gif)


## Why Choose MLflow?

* **ü™Ω FREE and OPEN** - MLflow is Open Source and 100% FREE. You don‚Äôt need to pay extra SaaS costs for adding observability to your GenAI stack. Your trace data is hosted on your own machine.

* **ü•á STANDARD** - MLflow Tracing is compatible with **OpenTelemetry**, an industry-standard observability spec. You can export your trace data to various services in your existing observability stack, such as Grafana, Prometheus, Datadog, New Relic, and more.

* **ü§ù Framework Support** - MLflow Tracing integrates with 15+ GenAI libraries, including OpenAI, LangChain, LlamaIndex, DSPy, and others. See the [Automatic Tracing](#automatic-tracing) section for the full list of supported libraries.

* **üîÑ End-to-End** - MLflow is designed for managing the end-to-end machine learning lifecycle. With its model tracking and evaluation capabilities, MLflow empowers you to leverage your trace data fully.

* **üë• Community** - MLflow boasts a vibrant Open Source community as a part of the Linux Foundation. With 19,000+ GitHub Stars and 15MM+ monthly downloads, MLflow is a trusted standard in the MLOps/LLMOps ecosystem.


## Introduction to Observability and Traces

:::tip

If you are new to the tracing or observability concepts, we recommend starting with the [Tracing Concepts](./ceoncept) page.

:::

Modern Generative AI applications often involve complex pipelines with multiple steps, APIs, and integrations. Observability provides visibility into these workflows, making it easier to:

- **Debugging bugs**: Identify the root cause of errors in your application.
- **Inspect quality**: Analyze inputs and outputs for unexpected behaviors.
- **Evaluate models**: Assess intermediate outputs to refine LLM workflows.
- **Monitor production**: Track performance and ensure reliability at scale.

MLflow Tracing automatically logs this data, enabling efficient monitoring, debugging, and optimization of your LLM applications.

## Automatic Tracing

MLflow Tracing is integrated with various GenAI libraries and provide **one-line automatic tracing** experience for each library (and the combination of them!). Click on the icon below to see detailed examples to integrate MLflow with your favorite library.

<CardGroup isSmall>
  <SmallLogoCard link="/llms/langchain/autologging">
    <span>![LangChain Logo](/images/logos/langchain-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/llms/langchain/autologging">
    <span>![LangGraph Logo](/images/logos/langgraph-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/llms/llama-index#enable-tracing">
    <span>![LlamaIndex Logo](/images/logos/llamaindex-logo.svg)</span>
  </SmallLogoCard>
  <SmallLogoCard link="#automatic-tracing">
    <span>![DSPy Logo](/images/logos/dspy-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/llms/openai/autologging">
    <span>![OpenAI Logo](/images/logos/openai-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/llms/openai/autologging#auto-tracing-for-openai-swarm">
    <span>![OpenAI Swarm Logo](/images/logos/openai-swarm-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="#automatic-tracing">
    <span>![AutoGen Logo](/images/logos/autogen-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="#automatic-tracing">
    <span>![Gemini Logo](/images/logos/google-gemini-logo.svg)</span>
  </SmallLogoCard>
  <SmallLogoCard link="#automatic-tracing">
    <span>![LiteLLM Logo](/images/logos/litellm-logo.jpg)</span>
  </SmallLogoCard>
  <SmallLogoCard link="#automatic-tracing">
    <span>![Anthropic Logo](/images/logos/anthropic-logo.svg)</span>
  </SmallLogoCard>
  <SmallLogoCard link="#automatic-tracing">
    <span>![CrewAI Logo](/images/logos/crewai-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="#automatic-tracing">
    <span>![Ollama Logo](/images/logos/ollama-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="#automatic-tracing">
    <span>![Instructor Logo](/images/logos/instructor-logo.svg)</span>
  </SmallLogoCard>
</CardGroup>
<br />


:::info Hint
Is your favorite library missing from the list? Consider [contributing to MLflow Tracing](./contribute) or [submitting a feature request](https://github.com/mlflow/mlflow/issues/new?assignees=&labels=enhancement&projects=&template=feature_request_template.yaml&title=%5BFR%5D) to our Github repository.
:::


## Tracing with SDK

In addition to the one-line auto tracing experience, MLflow offers Python SDK for manually instrumenting
you code and manipulating traces.

1. [Instrument a function with `@mlflow.trace` decorator.](sdk/manual-instrumentation.mdx#decorator)
2. [Instrument any block of code using `mlflow.start_span` context manager.](sdk/manual-instrumentation.mdx#context-manager)
3. [Setting a tag to an active trace]
4. [Enabling and disabling trace globally]

Please continue to the [MLflow Tracing SDK guide](sdk) for mode details.

## Query Traces

Trace data are useful for various downstream tasks, such as creating an evaluation dataset for offline evaluation and production monitoring.

MLflow provides several APIs to search and retrieve recorded traces programmatically. See [Searching and Retrieving Traces](sdk/search.mdx) for more details.

## Tracing in Production

MLflow Tracing is production ready. Read [Production Tracing](production.mdx) for the guidance to use it for monitoring models in production and various backend options.


:::note
MLflow Tracing support is available with the **MLflow 2.14.0** release.
:::