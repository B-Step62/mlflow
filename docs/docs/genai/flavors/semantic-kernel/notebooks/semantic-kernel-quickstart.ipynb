{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa1107a1",
   "metadata": {},
   "source": [
    "# Introduction to Using Semantic Kernel with MLflow\n",
    "\n",
    "Welcome to this interactive tutorial designed to introduce you to [SemanticKernel](https://learn.microsoft.com/en-us/semantic-kernel/overview/) and its integration with MLflow. This tutorial is structured as a notebook to provide a hands-on, practical learning experience that focuses on tracing.\n",
    "\n",
    "Note that MLflow currently only supports tracing support for Semantic Kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8f1241",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "First, we must install the required dependencies, enable MLflow autologging, and input an OpenAI API key. In this tutorial, we will leverage OpenAI for simplicity; however, other LLM providers can easily be leveraged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d032229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install mlflow -qU\n",
    "%pip install semantic_kernel openai nest_asyncio -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbfe4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Enable MLflow autologging for Semantic Kernel\n",
    "mlflow.semantic_kernel.autolog()\n",
    "\n",
    "# Set the OpenAI API key as an environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"openai_api_key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c8c6e1",
   "metadata": {},
   "source": [
    "### Quickstart\n",
    "\n",
    "Next, we will create a simple quickstart to show MLflow tracing. \n",
    "\n",
    "MLflow tracing logs granular telemetry to the MLflow servers, allowing debugging within agentic steps. At each agentic step, both Semantic Kernal and MLflow metadata is logged, allowing users to speed up their development and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38961dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/01 13:25:17 WARNING mlflow.tracing.processor.mlflow_v2: Creating a trace within the default experiment with id '0'. It is strongly recommended to not use the default experiment to log traces due to ambiguous search results and probable performance issues over time due to directory table listing performance degradation with high volumes of directories within a specific path. To avoid performance and disambiguation issues, set the experiment for your environment using `mlflow.set_experiment()` API.\n",
      "2025/07/01 13:25:18 WARNING mlflow.entities.span: Attributes must be a dictionary, but got <class 'mappingproxy'>. Skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI says: Whether sushi is the \"best\" food ever is highly subjective and depends on individual tastes and preferences. Many people love sushi for its unique flavors, textures, and the skill involved in its preparation. Sushi can also be seen as a healthier option, often featuring fresh fish and vegetables. \n",
      "\n",
      "However, others may prefer different cuisines or dishes based on their cultural background, dietary restrictions, or personal preferences. Ultimately, the \"best\" food is a matter of personal opinion, and there are countless culinary delights to explore around the world!\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio # This library is needed to run async calls in a Jupyter notebook\n",
    "\n",
    "import openai\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "from semantic_kernel.functions.function_result import FunctionResult\n",
    "\n",
    "# Allow nested event loops (needed in e.g. notebooks or certain test runners)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Create a basic OpenAI client\n",
    "openai_client = openai.AsyncOpenAI()\n",
    "\n",
    "# Create a Semantic Kernel instance and register the OpenAI chat completion service\n",
    "kernel = Kernel()\n",
    "kernel.add_service(\n",
    "    OpenAIChatCompletion(\n",
    "        service_id=\"chat-gpt\",\n",
    "        ai_model_id=\"gpt-4o-mini\",\n",
    "        async_client=openai_client,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Define an async function that invokes your prompt\n",
    "with mlflow.start_run(run_name=\"semantic_kernel simple example\"):\n",
    "    async def run_query() -> FunctionResult:\n",
    "        return await kernel.invoke_prompt(\"Is sushi the best food ever?\")\n",
    "\n",
    "# Call this via asyncio.run(), which is required for a Jupyter notebook environment.\n",
    "# If you are using a script, you can simply call `await run_query()`.\n",
    "answer = asyncio.run(run_query())\n",
    "print(\"AI says:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fbeaa0",
   "metadata": {},
   "source": [
    "### Explore Traces\n",
    "\n",
    "Next, let's open the MLflow UI and explore the logged traces. In the cell below, we will open up the MLflow UI in an iFrame for easy access.\n",
    "\n",
    "To find our trace, see the rendered iFrame below and follow these steps.\n",
    "1. Click on the MLflow experiment. If this is not the first time running the notebook, there may be multiple experiments. In this case, please find the most recently logged experiment. \n",
    "2. Click on your run of interest.\n",
    "3. Click on your trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9882e5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['mlflow', 'ui', '--port', '5000']>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "from IPython.display import IFrame\n",
    "\n",
    "# Start the MLflow UI in a background process\n",
    "mlflow_ui_command = [\"mlflow\", \"ui\", \"--port\", \"5000\"]\n",
    "subprocess.Popen(\n",
    "    mlflow_ui_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, preexec_fn=os.setsid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4451c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the MLflow server to start then run the following command\n",
    "# Note that cached results don't render, so you need to run this to see the UI\n",
    "IFrame(src=\"http://localhost:5000\", width=1000, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd44823f",
   "metadata": {},
   "source": [
    "If everything executed properly, you should see a single MLflow trace. The trace should have two spans:\n",
    "1. A top-level **parent** span with a name similar to `execute_tool bjmtaiGXxiTwWooE`. This top level span represents the semantic kernel invocation.\n",
    "2. A **child** span with our chat payload. This span represents the single call via our `OpenAIChatCompletion` semantic kernel service and contains robust metadata about the invocation. \n",
    "\n",
    "Given semantic kernel typically involves complex async agentic calls, MLflow tracing is an invaluable tool when determine how internal calls impact the overall Kernel invocation.\n",
    "\n",
    "For more about tracing, please see the [MLflow tracing docs](https://mlflow.org/docs/latest/genai/tracing)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
