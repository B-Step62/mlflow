---
sidebar_position: 14
sidebar_label: Semantic Kernel
---

import { APILink } from "@site/src/components/APILink";
import { Card, CardGroup, SmallLogoCard } from "@site/src/components/Card";
import TOCInline from "@theme/TOCInline";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Tracing txtai

# TODO
### ![Semantic Kernel Tracing via autolog](/images/llms/tracing/TODO.png)


[Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/overview/) is a lightweight, open-source SDK that functions as AI middleware, enabling you to integrate AI models into your C#, Python, or Java codebase via a uniform API layer. By abstracting model interactions, it lets you swap in new models without rewriting your application logic.  

[MLflow Tracing](/genai/tracing) provides automatic tracing capability for Semantic Kernel. Auto tracing for Semantic Kernel can be enabled by calling the `mlflow.semantic_kernel.autolog` function, MLflow will capture traces for LLM invocations and log them to the active MLflow Experiment.

To get started, lets install the requisite libraries. Note that we will use OpenAI for demonstration purposes, but this tutorial extends to all providers supported by Semantic Kernel.

```bash
pip install mlflow semantic_kernel openai
```

Then, enable autologging in your Python code:

```python
import mlflow

mlflow.semantic_kernel.autolog()
```

Finally, for setup, let's establish our OpenAI token:

```python
import os
from getpass import getpass

# Set the OpenAI API key as an environment variable
os.environ["OPENAI_API_KEY"] = getpass("openai_api_key: ")
```

### Examples

<Tabs>
  <TabItem value="basic" label="Simple Example" default>

  The simplest example to show the tracing integration is to instrument a [ChatCompletion Kernel](https://learn.microsoft.com/en-us/semantic-kernel/concepts/ai-services/chat-completion/?tabs=csharp-AzureOpenAI%2Cpython-AzureOpenAI%2Cjava-AzureOpenAI&pivots=programming-language-python).

  ```python
  import openai
  from semantic_kernel import Kernel
  from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion
  from semantic_kernel.functions.function_result import FunctionResult

  # Create a basic OpenAI client
  openai_client = openai.AsyncOpenAI()

  # Create a Semantic Kernel instance and register the OpenAI chat completion service
  kernel = Kernel()
  kernel.add_service(
    OpenAIChatCompletion(
      service_id="chat-gpt",
      ai_model_id="gpt-4o-mini",
      async_client=openai_client,
    )
  )

  # Define an async function that invokes your prompt
  async def run_query() -> FunctionResult:
    return await kernel.invoke_prompt("Is sushi the best food ever?")

  with mlflow.start_run(run_name="semantic_kernel simple example"):
    answer = await run_query()
    print("AI says:", answer)
  ```



  ### ![Semantic Kernel Chat Completion Tracing via autolog](/images/llms/tracing/TODO.png)

  </TabItem>
</Tabs>

