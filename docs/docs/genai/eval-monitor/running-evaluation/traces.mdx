# Evaluating Traces

--- INSTRUCTION TO AUTHOR ---
This guide shows how to evaluate production traces quality with MLflow.

Technology used:
- MLflow GenAI evaluation
- OpenAI
- MLflow Tracing

The steps will be
- Common setup (installation, connect to tracking server, refer to the quickstart guide)
- Create a prompt and simulate production traffic using tracing.
  - Nudge to product tracing guide page
- Annotate the traces using MLflow feedback tracking
- Define scorer that compare actual answer and annotated expected response
- Review the results in MLflow

--- END INSTRUCTION TO AUTHOR ---

<table style={{ width: "100%" }}>
  <thead>
    <tr>
      <th style={{ width: "30%" }}>Component</th>
      <th style={{ width: "70%" }}>What we use in this guide</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Dataset</strong><br/><small>Inputs &amp; expectations</small></td>
      <td>
        Traces logged in MLflow.
      </td>
    </tr>
    <tr>
      <td><strong>Scorer</strong><br/><small>Evaluation criteria</small></td>
      <td>
        Built-in scorers (Guidelines, RetrievalSufficiency), and custom scorers.
      </td>
    </tr>
    <tr>
      <td><strong>Predict Function</strong><br/><small></small></td>
      <td>
        N/A (traces already contain the outputs)
      </td>
    </tr>
  </tbody>
</table>
