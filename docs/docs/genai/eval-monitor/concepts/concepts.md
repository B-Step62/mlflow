<!-- # DRAFT NOTES - Evaluation Concepts Outline

This is a working document with notes about evaluation concepts. See the [concepts index](/genai/eval-monitor/concepts/index) for the official documentation.

---

From trace data model

- Trace has Assessments
- Assessments are either feedback or expectation

Evaluation Harness

- you can optionally pass a mlflow model identifier to link the results to your app versions
- there are 2 modes for eval harness
  - have eval harness call your app
    - takes your app's code, your eval dataset, and scorers
    - runs the app for each row in eval dataset, the app produces a trace
    - puts the trace in an evaluation run
    - calls each scorer for each trace, writing the feedback to the trace
  - use an "answer sheet" which is either existing traces from your app OR inputs/outputs dicts from your app
    - puts the trace in an evaluation run (if you pass traces)
    - if you pass inputs/outputs, it creates a trace for those and putting in the eval run
    - calls each scorer for each trace, writing the feedback to the trace
- we suggest using mode 1 bc
  - It ensures that your scorers function consistently in both development and production environments because they operate on traces generated by the same application code. MLflow can also parallelize the calls to your application, which significantly speeds up the evaluation process.
- however mode 2 is available for
  - This method is often referred to as "answer sheet" mode. It's available as an alternative if you don't have a way to run your application's code with tracing directly (as in method #1). This provides an escape hatch, allowing you to run your application elsewhere, gather its inputs and outputs, and then bring that data to MLflow for evaluation.
  - Using pre-computed outputs can lead to skew between your online (production) and offline (development) scorer results. This is because the traces generated by `evaluate()` in method #1 might capture more context or behave slightly differently than manually assembled inputs/outputs. Prefer method #1 whenever possible.
- more info docs/genai/eval-monitor/concepts/eval-harness

Evaluation UI

- lets you see individiual evaluation results
  - overview of all records & metrics from the scorers
  - click into each one to see the trace
- you can compare 2 evaluation runs to see how your qualtiy improved (or where it degressed)

Evaluation Run

- this is just an mlflow run
- it contains all the traces which have their assessments
- you can use this to track evals over time for app versions and evaluation datasets
- if you use mlflow tracking for your agent/app, you can link these to the app version

Scorers

- it takes a trace from your application + optionally expecations (if you have labels, only works with evaluate in offline eval)
- SUPER IMPORTANT: desgined so that you can write a scorer once and use it for offline eval and prod monitoring
- each scorer has some arbitrary logic to take the app's inputs/outputs + execptations and then compute 1+ Feedbacks that are quality assessments
- the logic is different for each scorer
  - some scorers just use python code to evaluate quality - determisitc checks like "the length of the response"
  - some scorers call to llm judges to do quality checks - these check the semantics of the language
- when you write a custom scorer, mlflow evaluate or the prod monitoring service can process the trace before hand and pass you the inputs/outputs directly. this is purely a convenience for you.
- we give you a set of predefined scorers that use our predefined llm judges
  - Typically, you can get started with evaluation using predfined scorers, but as your application logic and evaluation criteria gets more complex (or, your application's trace does not meet the scorer's requirements), you switch to wrapping the underlying judge in a [custom scorers](/genai/eval-monitor/custom-scorers) or creating a [custom LLM scorer](/genai/eval-monitor/custom-judge/index).
  - more info: docs/genai/eval-monitor/predefined-judge-scorers
- you can create custom scorers - this is expected for most applications

  - use our guidelines llm judge scorer
    - If your guidelines only consider the app's inputs and outputs and your app's trace only has simple inputs (e.g., only the user query) and outputs (e.g., only app response), use the [prebuilt guidelines scorer](#1-use-the-prebuilt-guidelines-scorer).
    - more info docs/genai/eval-monitor/custom-judge/meets-guidelines
  - wrap any of our llm judges (guidelines or the other prebuilt ones or use our prompt template judge) if you need more control over how the data is passed to the judge or for post-processing or doing multiple judges
    - more info docs/genai/eval-monitor/custom-judge/create-prompt-judge
  - write ANY custom python code

    - more info docs/genai/eval-monitor/custom-scorers

  - these are passed to the scorer by either evaluation harness OR prod monitoring service

Judges

- Judge - building block. it takes a set of input field that varies per judge, passes them to an LLM judge, and then returns a score and rationale in a feedback object. It is exposed via an SDK and designed to used as a building block.
- exposed as an API/SDK to use
- we invested a lot in tuning the quality of these with our research team
- there are a few types
  - use case specific - common eval use cases like groundedness, correctness, etc
  - guidelines - easy way to create pass/fail criteria
  - prompt based - for more customization and control
- the ways you COULD use it
  - wrap it in a scorer
  - call it from within your application itself to do judgements as part of the app's logic (e.g., check context reelevance before generating using that context)
- important to call out that

Production monitoring service

- this is like the production equivilent of evaluate() harness
- you give it scorers, and a smapling rate for each
- the service calls these scorers on your production traces, attaching the Feedback from the scorers to the traces in your experiment

Evaluation Dataset

- this is the set of represenstive user inputs to your app that you use for evaluation
- optionally it can have Expectations which are ground truth labels from domain experts
- you can use the evaluation dataset service, which gives you a way to track eval sets over time.
  - its stored as a delta table and gives you crud APIs on top.
  - also helps you convert existing traces into records, copying the inputs and expectatations from those traces + a link to the source trace for lineage.
  - maintains a history of who made the edits & last update times
- you can also just manually pass an evaluation dataset as one of the following

  - list of traces
  - array of dicts

- the managed dataset has the following schema

Evaluation datasets have the following schema:

| Column                  | Data Type | Description                                                                                                                                                                          |
| ----------------------- | --------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| dataset_record_id       | string    | The unique identifier for the record.                                                                                                                                                |
| inputs                  | string    | Inputs to evaluation as json serialized `dict<str, Any>`.                                                                                                                            |
| expectations            | string    | Expected values as json serialized `dict<str, Any>`. `expectations` has reserved keys that are used for LLM judges, such as `guidelines`, `expected_facts`, and `expected_response`. |
| create_time             | timestamp | The time when the record was created.                                                                                                                                                |
| created_by              | string    | The user who created the record.                                                                                                                                                     |
| last_update_time        | timestamp | The time when the record was last updated.                                                                                                                                           |
| last_updated_by         | string    | The user who last updated the record.                                                                                                                                                |
| source                  | struct    | The source of the dataset record.                                                                                                                                                    |
| source.human            | struct    | Defined when the source is from a human.                                                                                                                                             |
| source.human.user_name  | string    | The name of the user associated with the record.                                                                                                                                     |
| source.document         | string    | Defined when the record was synthesized from a doc.                                                                                                                                  |
| source.document.doc_uri | string    | The URI of the document.                                                                                                                                                             |
| source.document.content | string    | The content of the document.                                                                                                                                                         |
| source.trace            | string    | Defined when the record was created from a trace.                                                                                                                                    |
| source.trace.trace_id   | string    | The unique identifier for the trace.                                                                                                                                                 |
| tags                    | map       | Key-value tags for the dataset record.                                                                                                                                               | -->
