---
description: 'LLM judges in MLflow - AI-powered evaluation functions with code examples and patterns'
last_update:
  date: 2025-05-18
---

# LLM-based scorers

## Overview

Judges are MLflow's SDK/API building blocks for LLM-based quality assessment. Each judge uses a [specially tuned, Databricks-hosted LLM model](#llm-judge-trust) designed to perform GenAI quality assessments.

Think of a judge as an AI assistant specialized in quality assessment - they read your app's outputs and make assessments based on criteria you define. For example, they can understand that `give me healthy food options` is the same query as `food to keep me fit` are very similar queries.

:::important
While judges can be used as standalone APIs, judges must be wrapped in [Scorers](/genai/eval-monitor/concepts/scorers) for use by the [Evaluation Harness](/genai/eval-monitor/concepts/eval-harness) and [production monitoring service](/genai/eval-monitor/concepts/production-monitoring).
:::

## When to use judges

Use judges when you need to evaluate plain language inputs or outputs:

- **Semantic correctness**: "Does this answer the question correctly?"
- **Style and tone**: "Is this appropriate for our brand voice?"
- **Safety and compliance**: "Does this follow our content guidelines?"
- **Relative quality**: "Which response is more helpful?"

Use [custom, code-based scorers](/genai/eval-monitor/concepts/scorers) instead for:

- **Exact matching**: Checking for specific keywords
- **Format validation**: JSON structure, length limits
- **Performance metrics**: Latency, token usage

## Deeper dive into judges

For detailed information about specific judges:

### Predefined judges

MLflow provides research-validated judges for common use cases:

```python
from mlflow.genai.judges import (
    is_safe,  # Content safety
    is_relevant,  # Query relevance
    is_grounded,  # RAG grounding
    is_correct,  # Factual accuracy
    is_context_sufficient,  # Retrieval quality
)
```

See [predefined judges reference](/genai/eval-monitor/concepts/judges/pre-built-judges-scorers) for detailed documentation.

### Custom judges

Build domain-specific judges using two approaches:

1. **[Guidelines-based](/genai/eval-monitor/concepts/judges/guidelines)** _(recommended starting point)_ - Natural language pass/fail criteria that are easy to explain to stakeholders. Best for compliance checks, style guides, or information inclusion/exclusion.

2. **[Prompt-based](/genai/eval-monitor/concepts/judges/prompt-based-judge)** - Full prompt customization for complex evaluations. Use when you need multiple output values (e.g., "great", "ok", "bad") or criteria that can't be expressed as pass/fail guidelines.

## Judge accuracy

Databricks continuously improves judge quality through:

- **Research validation** against human expert judgment
- **Metrics tracking**: Cohen's Kappa, accuracy, F1 score
- **Diverse testing** on academic and real-world datasets

See [Databricks blog on LLM judge improvements](https://www.databricks.com/blog/databricks-announces-significant-improvements-built-llm-judges-agent-evaluation) for details.

::include[llm-judge-trust]

## Next steps

### How-to guides

- [Use predefined LLM scorers](/genai/eval-monitor/predefined-judge-scorers) that wrap built-in judges
- [Create guideline-based judges](/genai/eval-monitor/custom-judge/meets-guidelines) using natural language criteria
- [Build custom prompt-based judges](/genai/eval-monitor/custom-judge/create-prompt-judge) for complex evaluation
<!---- - [Align judges with domain experts](/genai/eval-monitor/custom-judge/tune-judge) to match human judgment  -->

### Concepts

- [Predefined judges reference](/genai/eval-monitor/concepts/judges/pre-built-judges-scorers) - Detailed documentation of all built-in judges
- [Guidelines-based judges](/genai/eval-monitor/concepts/judges/guidelines) - How guideline evaluation works
- [Prompt-based judges](/genai/eval-monitor/concepts/judges/prompt-based-judge) - Creating custom evaluation prompts
- [Scorers](/genai/eval-monitor/concepts/scorers) - How judges integrate with the evaluation system
